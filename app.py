import datetime
import json
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.support.ui import Select # Select
from selenium.webdriver.common.by import By 
from  selenium.webdriver.support.ui  import  WebDriverWait 
from  selenium.webdriver.support  import  expected_conditions  as  EC
import pymysql

# å°‡æ•´ç†çš„å€åŸŸå¯«æˆHASH MAP
def write_Map_JSON():
    regions_path = "./json/regions.json"
    with open(regions_path, 'r', encoding="utf8") as json_file: # è®€å– JSONæª”æ¡ˆ
        regions_data = json.load(json_file) # regionsè®Šæ•¸ç‚ºæ‰€æœ‰è¦æŠ“å–é„‰é®è³‡è¨Š

    map_regions = dict()
    for region in regions_data:
        city = region['city']
        if city not in map_regions:
            map_regions[city] = list()
        map_regions[city].append(region)
    
    to_path = f"json/map_regions.json"
    json_object = json.dumps(map_regions, indent=4, ensure_ascii=False)
    with open(to_path, "w", encoding="utf8") as outfile:
        outfile.write(json_object)
    
    return json_object


"""ç«‹å³è³‡è¨Š"""
def getNow(soup):
    C_weather_table = soup.find("table", {"class": "cubeV9-table"})
    tbody = C_weather_table.find("tbody")
    tds = tbody.find_all("td")
    C_Weather = dict()
    for td in tds:
        span = td.find("span")
        class_name = span['class'][0]

        # é€™é‚Šå› ç‚ºæº«åº¦éƒ¨åˆ†æœ‰åˆ† ã€æ”æ°æº«åº¦ã€åŠã€è¯æ°æº«åº¦ã€ã€‚å› æ­¤<span>åº•ä¸‹é‚„æœ‰å€‹<span>
        if( (class_name == 'GT_T') | (class_name == 'GT_AT')):
            C_Weather[class_name] = span.find("span", {"class", "is-active"}).getText()
            continue

        C_Weather[class_name] = span.getText()
    return C_Weather

"""é€ä¸‰å°æ™‚é å ±"""
def getThreeHours(soup):
    three_hours_weather_table = soup.find("table", {"id": "TableId3hr"})
    total = dict()

    #region (è™•ç†è¡¨é ­) => æ³¨æ„æœ‰ colspanå•é¡Œ
    thead = three_hours_weather_table.find("thead")
    ths = thead.find_all("th", {"headers": "PC3_D"})
    for th in ths:
        header_name = th["headers"][0]
        if( header_name not in total ):
            total[header_name] = []
        colspan = th["colspan"] if (th.has_attr("colspan")) else 1 # åˆ¤æ–·è©²æ—¥æœŸè·¨è¶³äº†å¹¾å€‹æ™‚é–“å€é–“(å¦‚æœä¸å­˜åœ¨colaspanä»£è¡¨åªæœ‰ä¸€æ¬„å°±å¼„1å°±å¥½)
        total[header_name].extend( [ th.getText() for i in range(0, int(colspan)) ] ) # ä¾ç…§è·¨è¶³æ™‚é–“å€é–“çš„æ•¸é‡é€éä¸€è¡Œè¿´åœˆä¾†pushåˆ°é™£åˆ—ä¸­
    
    tbody = three_hours_weather_table.find("tbody")
    trs = tbody.find_all("tr")
    #endregion

    #region (è™•ç†è¡¨æ ¼å…§å®¹)
    for tr in trs:
        ths = tr.find_all("th", {"headers": "PC3_Ti"}) # æŠ“åˆ°æ‰€æœ‰<th headers="PC3_Ti"></th>
        for th in ths:
            header_name = th["headers"][0]
            if( header_name not in total ):
                total[header_name] = []
            total[header_name].append(th.getText())

        tds = tr.find_all("td")
        for td in tds:
            header_name = td["headers"][0]
            if(  header_name not in total ): # ç•¶ç›®å‰Dictä¸­ç„¡è©²keyæ™‚ï¼Œç«‹å³å®šç¾©
                total[header_name] = []

            # å¤©æ°£èªªæ˜: åªæŠ“æ–‡å­—èªªæ˜
            if(header_name == "PC3_Wx"): 
                img = td.find("img")
                total[header_name].append(img['alt'])
                continue

            # æº«åº¦: åº•ä¸‹é‚„æœ‰åˆ†ã€æ”æ°æº«åº¦ã€åŠã€è¯æ°æº«åº¦ã€
            if( (header_name == 'PC3_T') | (header_name == 'PC3_AT') ): 
                temp = td.find("span", {"class", "is-active"}).getText()
                total[header_name].append(temp)
                continue

            # é™é›¨æ©Ÿç‡: å› ç‚ºé™é›¨æ©Ÿç‡é€™é‚Šæœƒä»¥å…©å€‹å€é–“ç‚ºä¸€å€‹å€¼
            if(header_name == "PC3_Po"): 
                if( td.has_attr('colspan') ): # å¦‚æœè©²æ¬„ç‚ºå…©æ¬„å¼å‰‡è¦ä¸€æ¬¡pushå…©ç­†
                    total[header_name].extend( [td.getText(), td.getText()] ) 
                    continue

            total[header_name].append(td.getText())
    #endregion

    # print(total)
    result = list()
    keys = total.keys()
    for index in range(0, len(total["PC3_Ti"])): # é–‹å§‹æŠ“å–å„æ™‚é–“é»çš„é æ¸¬æ•¸æ“š
        list_key = list() # ç”¨ä¾†è¨˜éŒ„å–®ä¸€æ™‚é–“é»çš„æ‰€æœ‰å¤©æ°£æ•¸æ“š
        for key in keys: # é€ékeyså»åšä¸åŒæ•¸æ“šçš„åˆ‡æ›
            list_key.append( total[key][index] )
        zip_dict = dict( zip(keys, list_key) ) # é€ézipå°‡å…©å€‹Listè½‰æ›æˆdictã€‚
        result.append(zip_dict)
    return result

"""ä¸€é€±é å ±(å¯«æ³•è·Ÿé€ä¸‰å°æ™‚é å ±å·®ä¸å¤š)"""
def getSevenDays(soup):
    table = soup.find("table", {"id": "TableIdweeks"})
    total = dict()

    #region (è™•ç†è¡¨é ­) => æ³¨æ„æœ‰ colspanå•é¡Œ
    thead = table.find("thead")
    D_ths = thead.find_all("th", {"headers": "PC7_D"})
    for th in D_ths:
        header_name = th["headers"][0]
        if( header_name not in total ):
            total[header_name] = []
        colspan = th["colspan"] if (th.has_attr("colspan")) else 1 # åˆ¤æ–·è©²æ—¥æœŸè·¨è¶³äº†å¹¾å€‹æ™‚é–“å€é–“(å¦‚æœä¸å­˜åœ¨colaspanä»£è¡¨åªæœ‰ä¸€æ¬„å°±å¼„1å°±å¥½)
        date = th.getText()
        date = date.split("æ˜Ÿ")[0] # é€™é‚Šå› ç‚ºæŠ“åˆ°çš„è³‡æ–™æœƒæ˜¯  "<XX/XX>æ˜ŸæœŸX"ã€‚æˆ‘åªéœ€è¦å‰é¢çš„æ—¥æœŸè€Œå·²
        date = datetime.datetime.strptime( f"{ datetime.date.today().year }/{date}", "%Y/%m/%d")
        date = datetime.datetime.strftime( date, "%Y-%m-%d" )
        total[header_name].extend( [ date for i in range(0, int(colspan)) ] ) # ä¾ç…§è·¨è¶³æ™‚é–“å€é–“çš„æ•¸é‡é€éä¸€è¡Œè¿´åœˆä¾†pushåˆ°é™£åˆ—ä¸­
    #endregion

    #region (è™•ç†è¡¨æ ¼å…§å®¹)
    tbody = table.find("tbody")
    trs = tbody.find_all("tr")
    for tr in trs:
        ths = tr.find_all("th", {"headers": "PC7_Ti"}) # æŠ“åˆ°æ‰€æœ‰<th headers="PC7_Ti"></th>
        for th in ths:
            header_name = th["headers"][0]
            if( header_name not in total ):
                total[header_name] = []
            total[header_name].append(th.getText())

        tds = tr.find_all("td")
        for td in tds:
            header_name = td["headers"][0]
            if(  header_name not in total ): # ç•¶ç›®å‰Dictä¸­ç„¡è©²keyæ™‚ï¼Œç«‹å³å®šç¾©
                total[header_name] = []
            if(header_name == "PC7_Wx"): # å¤©æ°£èªªæ˜åªæŠ“æ–‡å­—èªªæ˜
                img = td.find("img")
                total[header_name].append(img['alt'])

                continue
            if( (header_name == 'PC7_MaxT') or (header_name == 'PC7_MinT') or (header_name == 'PC7_MaxAT') or (header_name == 'PC7_MinAT')): # æº«åº¦åº•ä¸‹é‚„æœ‰åˆ†ã€æ”æ°æº«åº¦ã€åŠã€è¯æ°æº«åº¦ã€
                temp = td.find("span", {"class", "is-active"}).getText()
                total[header_name].append(temp)
                continue
            
            if(header_name == "PC7_UVI"):
                uvi = td.find("span").getText()
                total[header_name].extend( [uvi, uvi] )
                continue

            total[header_name].append(td.getText())
    #endregion


    result = list()
    keys = total.keys()
    for index in range(0, len(total["PC7_D"])): # é–‹å§‹æŠ“å–å„æ™‚é–“é»çš„é æ¸¬æ•¸æ“š
        list_key = list() # ç”¨ä¾†è¨˜éŒ„å–®ä¸€æ™‚é–“é»çš„æ‰€æœ‰å¤©æ°£æ•¸æ“š
        for key in keys: # é€ékeyså»åšä¸åŒæ•¸æ“šçš„åˆ‡æ›
            list_key.append( total[key][index] )
        zip_dict = dict( zip(keys, list_key) ) # é€ézipå°‡å…©å€‹Listè½‰æ›æˆdictã€‚
        result.append(zip_dict)

    return result

# é€£ç·š
def connect_db(host, user, pwd, dbname, port):
    try:
        db = pymysql.connect(
            host = host,
            user = user,
            passwd = pwd,
            database = dbname,
            port = int(port)
        )
        # print("é€£ç·šæˆåŠŸ")
        return db
    except Exception as e:
        print('é€£ç·šè³‡æ–™åº«å¤±æ•—: {}'.format(str(e)))
    return None

# çˆ¬èŸ²
def Crawler(url, regions):
    try:
    #region (chromedriver è¨­å®š)
        option = webdriver.ChromeOptions() # ChromeDriver Options
        # ã€åƒè€ƒã€‘https://ithelp.ithome.com.tw/articles/10244446
        option.add_argument("headless") # ä¸é–‹ç¶²é æœå°‹
        option.add_argument('blink-settings=imagesEnabled=false') # ä¸åŠ è¼‰åœ–ç‰‡æé«˜æ•ˆç‡
        option.add_argument('--log-level=3') # é€™å€‹optionå¯ä»¥è®“ä½ è·Ÿheadlessæ™‚ç¶²é ç«¯çš„console.logèªªæ°æ°
        """ä¸‹é¢åƒæ•¸èƒ½æå‡çˆ¬èŸ²ç©©å®šæ€§"""
        option.add_argument('--disable-dev-shm-usage') # ä½¿ç”¨å…±äº«å…§å­˜RAM
        option.add_argument('--disable-gpu') # è¦é¿éƒ¨åˆ†chrome gpu bug

        driver = webdriver.Chrome(chromedriver_path, chrome_options=option) #å•Ÿå‹•æ¨¡æ“¬ç€è¦½å™¨
    #endregion

    #region (å•Ÿå‹• chromedriver)
        print("=======================================")
        print(f"ğŸ’–ã€{city}ã€‘ ({count}/{len(map_regions)})")

        driver.get(url) #å–å¾—ç¶²é ä»£ç¢¼
        # if not driver.title:
        #     print(f"ğŸ“›æœªæˆåŠŸé€²å…¥é é¢...")
        #     pass
            
        # print(f"âœ…æˆåŠŸé€²å…¥é é¢...({driver.title})")
    #endregion

    #region (é¸å–®è™•ç† + åˆ‡æ›è¿‘ä¸‰å°æ™‚ / ä¸€å‘¨)
        # ç¸£å¸‚é¸å–®
        select_County = WebDriverWait(driver, 10, 1).until(
            EC.presence_of_element_located(
                (By.XPATH, '//*[@id="CountySelect"]')
            )
        )
        select_County = Select( select_County )
        
        # å€åŸŸé¸å–®
        select_TID = WebDriverWait(driver, 10, 1).until(
            EC.presence_of_element_located(
                (By.XPATH, '//*[@id="TID"]')
            )
        )
        select_TID = Select( select_TID )


        # ã€åˆ‡æ› è¿‘ä¸‰å°æ™‚é æ¸¬ / ä¸€å‘¨é æ¸¬ã€‘
        three_hours_aTag = driver.find_element(By.XPATH, '//*[@id="Tab_3hrTable"]')
        one_week_aTag = driver.find_element(By.XPATH, '//*[@id="Tab_weeksTable"]')

        one_week_aTag.click() # åˆ‡æ›åˆ°ä¸€å‘¨
    #endregion

        # é–‹å§‹åˆ‡æ›é„‰é®å€
        for i, region in enumerate(regions):
        # for i, region in enumerate(regions[0:1]):
            print("-----")
            print(f"ç›®å‰é€²åº¦: {region['city'] + region['district']}({i+1}/{len(regions)})")
            # å¾—åˆ°ç•¶å‰å€åŸŸå¤©æ°£
            select_TID.select_by_value(region["ID"])

            # é€é soupè§£æ XML
            soup = BeautifulSoup(driver.page_source, "lxml")

            #region (å³æ™‚è³‡è¨Š)
            try:
                # ç¢ºå®šæœ‰é€™å€‹è³‡æ–™å†é–‹å§‹æŠ“
                GT_Time = WebDriverWait(driver, 10, 1).until(
                    EC.presence_of_element_located(
                        (By.CLASS_NAME, 'GT_Time')
                    )
                )
                now_info = getNow(soup) 
                # print(f"æº«åº¦: {now_info['GT_T']} ï¼› æ¿•åº¦: {now_info['GT_RH']}")
                now_info = json.dumps(now_info, indent = 4)
            except Exception as e:
                print(f"{region['city']}{region['district']}æŠ“å–å³æ™‚è³‡è¨Šæ™‚ç™¼ç”Ÿå•é¡Œ : {e}")
                continue
            #endregion

            

            #region (é€ä¸‰å°æ™‚é å ±)
            try:
                # ç¢ºå®šæœ‰é€™å€‹è³‡æ–™å†é–‹å§‹æŠ“
                TableId3hr = WebDriverWait(driver, 10, 1).until(
                    EC.presence_of_element_located(
                        (By.ID, 'TableId3hr')
                    )
                )
                three_hours = getThreeHours(soup) 
                three_hours = json.dumps(three_hours, indent = 4)
            except Exception as e:
                print(f"{region['city']}{region['district']}æŠ“å–é€ä¸‰å°æ™‚é å ±æ™‚ç™¼ç”Ÿå•é¡Œ : {e}")
                continue
            finally:
                one_week_aTag.click() # åˆ‡æ›åˆ°ä¸€å‘¨
            #endregion

            

            #region (ä¸€é€±é å ±)
            try:
                # ç¢ºå®šæœ‰é€™å€‹è³‡æ–™å†é–‹å§‹æŠ“
                TableIdweeks = WebDriverWait(driver, 10, 1).until(
                    EC.presence_of_element_located(
                        (By.ID, 'TableIdweeks')
                    )
                )
                seven_days = getSevenDays(soup) 
                seven_days = json.dumps(seven_days, indent = 4)
            except Exception as e:
                print(f"{region['city']}{region['district']}æŠ“å–ä¸€é€±é å ±æ™‚ç™¼ç”Ÿå•é¡Œ : {e}")
                continue
            finally:
                three_hours_aTag.click() # åˆ‡æ›åˆ°é€ä¸‰å°æ™‚é å ±
            #endregion

            #region (è³‡æ–™åº«å¯«å…¥)
            with db.cursor() as cursor: # è³‡æ–™è™•ç†å¥½å¾Œï¼Œé€²è¡Œè³‡æ–™åº«æ–°å¢å‹•ä½œ
                cursor.execute(
                    """INSERT INTO `weather_region` 
                        (`id`, `city`, `district`, `now_info`, `threehours`, `sevendays`) 
                        VALUES (%s, %s, %s, %s, %s, %s) 
                    ON DUPLICATE KEY UPDATE now_info = %s, threehours = %s, sevendays = %s""",
                    (
                        region['ID'], region['city'], region['district'], now_info, three_hours, seven_days , 
                        now_info, three_hours, seven_days,
                    )
                )
                db.commit() # å„²å­˜è®Šæ›´
            #endregion
        
    except KeyboardInterrupt:
        print("----(å·²ä¸­æ–·ç¨‹å¼)----")
    
    except Exception as e:
        print(f"ç™¼ç”Ÿå•é¡Œ: {e}")

    finally:
        print("----(ChromeDriverå·²é—œé–‰)----")
        driver.close()
        driver.quit()




if __name__ == '__main__':
    final_start = time.time()

    db = connect_db(
        host='127.0.0.1',
        user='root',
        pwd='Ru,6e.4vu4wj/3',
        dbname='greenhouse',
        port=3306,
    ) # è³‡æ–™åº«é€£ç·š

    if( not db ):
        print("è³‡æ–™åº«é€£ç·šç™¼ç”Ÿå•é¡Œ")
    

    #region (!è£½ä½œ hash map)
    # json_object = write_Map_JSON()
    # print(json_object)
    #endregion
    # start = time.time() # ç´€éŒ„é–‹å§‹æ™‚é–“(æœ€çµ‚è¦å¾—åˆ°æ•´å€‹åŸ·è¡Œéç¨‹ç¸½å…±èŠ±è²»å¤šå°‘æ™‚é–“)

    try:
        regions_path = "./json/map_regions.json"
        with open(regions_path, 'r', encoding="utf8") as json_file: # è®€å– JSONæª”æ¡ˆ
            map_regions = json.load(json_file) # regionsè®Šæ•¸ç‚ºæ‰€æœ‰è¦æŠ“å–é„‰é®è³‡è¨Š

        chromedriver_path = './chromedriver.exe'  # chromedriver
        count = 1
        

        
        # ã€å–®ä¸€ç¸£å¸‚æ¸¬è©¦ã€‘
        # city = 'åŸºéš†å¸‚'
        # regions = map_regions[city]
        # url = f'https://www.cwb.gov.tw/V8/C/W/Town/Town.html?TID={regions[0]["ID"]}' # å¾ç¬¬ä¸€ç­†é–‹å§‹æŠ“
        # Crawler(url, regions)
        # print(f"ã€çˆ¬èŸ²ã€‘ç¸½èŠ±è²»æ™‚é–“: {format( time.time() - start)}ç§’")

        # ã€å…¨ç¸£å¸‚æ¸¬è©¦ã€‘
        for city, regions in map_regions.items():
            url = f'https://www.cwb.gov.tw/V8/C/W/Town/Town.html?TID={regions[0]["ID"]}' # å¾ç¬¬ä¸€ç­†é–‹å§‹æŠ“
            Crawler(url, regions)
            count +=1
    
    except Exception as e:
        pass
    
    finally:
        
        
        print(f"ç¨‹å¼å·²ç¶“åŸ·è¡Œå®Œç•¢ï¼Œç¸½èŠ±è²»æ™‚é–“: {format( time.time() - final_start)}ç§’")
        print("è¦–çª—å°‡åœ¨å…©ç§’å¾Œå°‡é—œé–‰...")
        time.sleep(2)
    

